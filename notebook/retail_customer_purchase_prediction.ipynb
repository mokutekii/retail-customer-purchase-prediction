{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Retail Customer Purchase Prediction\n",
        "**COSC 4368 — Fundamentals of AI**\n",
        "\n",
        "Team: Matthew Nguyen, Benjamin Tran, Victor Bui, Gustavo Buenrostro  \n",
        "Date: (auto)\n",
        "\n",
        "## Abstract (fill after results)\n",
        "Brief problem, approach, and key result (ROC-AUC, F1).\n",
        "\n",
        "## Acknowledgments\n",
        "Instructor, peers; dataset: UCI Online Shoppers Intention (primary), UCI Online Retail II (optional RFM).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1) Reproducibility & environment\n",
        "import os, sys, json, time, math, numpy as np, pandas as pd\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import sklearn\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, learning_curve\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import (\n",
        "    roc_auc_score, f1_score, precision_score, recall_score, confusion_matrix,\n",
        "    roc_curve, auc, precision_recall_curve, classification_report, brier_score_loss\n",
        ")\n",
        "from sklearn.calibration import calibration_curve\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "\n",
        "# project paths (repo root → this notebook under notebooks/)\n",
        "ROOT = Path().resolve().parents[0]  # Assumes notebook is in notebook/ subdirectory\n",
        "DATA_DIR = ROOT / \"data\"\n",
        "OUT_FIG_BASE = ROOT / \"outputs\" / \"figures\"\n",
        "OUT_TAB_BASE = ROOT / \"outputs\" / \"tables\"\n",
        "OUT_FIG_BASE.mkdir(parents=True, exist_ok=True)\n",
        "OUT_TAB_BASE.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "RUN_STAMP = time.strftime('%Y%m%d-%H%M%S')\n",
        "OUT_FIG = OUT_FIG_BASE / RUN_STAMP\n",
        "OUT_TAB = OUT_TAB_BASE / RUN_STAMP\n",
        "OUT_FIG.mkdir(parents=True, exist_ok=True)\n",
        "OUT_TAB.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(\"Python:\", sys.version)\n",
        "print(\"pandas:\", pd.__version__)\n",
        "print(\"numpy:\", np.__version__)\n",
        "print(\"sklearn:\", sklearn.__version__)\n",
        "print(\"Save figs →\", OUT_FIG)\n",
        "print(\"Save tables →\", OUT_TAB)\n",
        "plt.rcParams[\"figure.dpi\"] = 150\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) Data (primary) & feature plan\n",
        "Primary: UCI Online Shoppers (`Revenue` is the label).  \n",
        "Optional: Online Retail II → RFM enrichment later.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load primary dataset\n",
        "TARGET = \"Revenue\"\n",
        "df = pd.read_csv(DATA_DIR / \"online_shoppers_intention.csv\")\n",
        "if df[TARGET].dtype != int and df[TARGET].dtype != \"int64\":\n",
        "    df[TARGET] = df[TARGET].astype(int)\n",
        "df.shape, df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Basic audit\n",
        "audit = pd.DataFrame({\n",
        "    \"dtype\": df.dtypes.astype(str),\n",
        "    \"n_null\": df.isnull().sum(),\n",
        "    \"n_unique\": df.nunique()\n",
        "}).sort_index()\n",
        "audit.to_csv(OUT_TAB / \"data_audit.csv\")\n",
        "audit.head(20)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Feature groups (plan)**\n",
        "- Behavioral: Administrative, Administrative_Duration, Informational, Informational_Duration,\n",
        "  ProductRelated, ProductRelated_Duration, BounceRates, ExitRates, PageValues, SpecialDay  \n",
        "- Temporal/Tech: Month, Weekend, OperatingSystems, Browser, Region, TrafficType, VisitorType  \n",
        "- RFM (optional extension)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) EDA & leakage audit\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Class balance\n",
        "counts = df[TARGET].value_counts().sort_index()\n",
        "counts.to_csv(OUT_TAB / \"class_counts.csv\")\n",
        "ax = counts.plot(kind=\"bar\", title=\"Class Counts\")\n",
        "fig = ax.get_figure(); fig.tight_layout(); fig.savefig(OUT_FIG / \"class_counts.png\"); plt.close(fig)\n",
        "counts, (counts / counts.sum()).rename(\"proportion\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Purchase rate by Month and VisitorType (if present)\n",
        "if \"Month\" in df.columns:\n",
        "    pr_month = df.groupby(\"Month\")[TARGET].mean().sort_values()\n",
        "    ax = pr_month.plot(kind=\"bar\", title=\"Purchase Rate by Month\")\n",
        "    fig = ax.get_figure(); fig.tight_layout(); fig.savefig(OUT_FIG / \"purchase_rate_by_month.png\"); plt.close(fig)\n",
        "\n",
        "if \"VisitorType\" in df.columns:\n",
        "    pr_vtype = df.groupby(\"VisitorType\")[TARGET].mean().sort_values()\n",
        "    ax = pr_vtype.plot(kind=\"bar\", title=\"Purchase Rate by VisitorType\")\n",
        "    fig = ax.get_figure(); fig.tight_layout(); fig.savefig(OUT_FIG / \"purchase_rate_by_visitortype.png\"); plt.close(fig)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Leakage guardrails:** only use features known by session end; all transforms happen inside `Pipeline(pre, clf)` fit on **train only**.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4) Preprocessing & split\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Identify numeric and categorical columns\n",
        "NUM_COLS = df.drop(columns=[TARGET]).select_dtypes(include=[\"int64\",\"float64\",\"int32\",\"float32\"]).columns.tolist()\n",
        "CAT_COLS = [c for c in df.drop(columns=[TARGET]).columns if c not in NUM_COLS]\n",
        "\n",
        "pre = ColumnTransformer([\n",
        "    (\"num\", StandardScaler(), NUM_COLS),\n",
        "    (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), CAT_COLS)\n",
        "])\n",
        "\n",
        "# Split: 70/15/15 stratified\n",
        "X = df.drop(columns=[TARGET])\n",
        "y = df[TARGET].astype(int)\n",
        "\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(\n",
        "    X, y, test_size=0.30, stratify=y, random_state=SEED\n",
        ")\n",
        "X_val, X_test, y_val, y_test = train_test_split(\n",
        "    X_temp, y_temp, test_size=0.50, stratify=y_temp, random_state=SEED\n",
        ")\n",
        "len(X_train), len(X_val), len(X_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# small utilities\n",
        "from collections import OrderedDict\n",
        "\n",
        "def fit_eval(pipe, name, Xtr=X_train, ytr=y_train, Xv=X_val, yv=y_val):\n",
        "    pipe.fit(Xtr, ytr)\n",
        "    if hasattr(pipe.named_steps[\"clf\"], \"predict_proba\"):\n",
        "        y_score = pipe.predict_proba(Xv)[:,1]\n",
        "    else:\n",
        "        y_score = pipe.decision_function(Xv)\n",
        "    y_pred = (y_score >= 0.5).astype(int)\n",
        "    met = OrderedDict(\n",
        "        model=name,\n",
        "        roc_auc_val=roc_auc_score(yv, y_score),\n",
        "        f1_val=f1_score(yv, y_pred),\n",
        "        precision_val=precision_score(yv, y_pred, zero_division=0),\n",
        "        recall_val=recall_score(yv, y_pred)\n",
        "    )\n",
        "    return pipe, met\n",
        "\n",
        "val_rows = []\n",
        "fitted = {}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5) Baselines (classical ML)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Logistic Regression (interpretable anchor)\n",
        "lr_pipe = Pipeline([(\"pre\", pre), (\"clf\", LogisticRegression(max_iter=1000, class_weight=\"balanced\", random_state=SEED))])\n",
        "lr_pipe, m = fit_eval(lr_pipe, \"LogisticRegression\")\n",
        "fitted[\"LogisticRegression\"] = lr_pipe; val_rows.append(m)\n",
        "pd.DataFrame(val_rows)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Decision Tree (quick depth sweep)\n",
        "best_dt, best_m_dt = None, None\n",
        "for d in [3,5,10,None]:\n",
        "    dt = Pipeline([(\"pre\", pre), (\"clf\", DecisionTreeClassifier(max_depth=d, random_state=SEED))])\n",
        "    dt, mdict = fit_eval(dt, f\"DecisionTree(max_depth={d})\")\n",
        "    if best_m_dt is None or mdict[\"roc_auc_val\"] > best_m_dt[\"roc_auc_val\"]:\n",
        "        best_dt, best_m_dt = dt, mdict\n",
        "fitted[\"DecisionTree\"] = best_dt; val_rows.append(best_m_dt)\n",
        "val_df = pd.DataFrame(val_rows).sort_values(\"roc_auc_val\", ascending=False)\n",
        "val_df.to_csv(OUT_TAB / \"validation_metrics.csv\", index=False)\n",
        "val_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Validation confusion matrix for LR\n",
        "pipe = fitted[\"LogisticRegression\"]\n",
        "if hasattr(pipe.named_steps[\"clf\"], \"predict_proba\"):\n",
        "    y_score = pipe.predict_proba(X_val)[:,1]\n",
        "else:\n",
        "    y_score = pipe.decision_function(X_val)\n",
        "y_pred = (y_score >= 0.5).astype(int)\n",
        "cm = confusion_matrix(y_val, y_pred)\n",
        "fig = plt.figure(); ax = fig.add_subplot(111)\n",
        "ax.imshow(cm, interpolation=\"nearest\")\n",
        "ax.set_title(\"Validation Confusion Matrix — LogisticRegression\")\n",
        "ax.set_xlabel(\"Predicted\"); ax.set_ylabel(\"Actual\")\n",
        "for (i,j), v in np.ndenumerate(cm):\n",
        "    ax.text(j, i, str(v), ha=\"center\", va=\"center\")\n",
        "fig.tight_layout(); fig.savefig(OUT_FIG / \"confusion_matrix_val_logreg.png\"); plt.close(fig)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Top LR coefficients (absolute) using feature names from fitted preprocessor\n",
        "pre_fit = pre.fit(X_train, y_train)\n",
        "try:\n",
        "    cat_names = list(pre_fit.named_transformers_[\"cat\"].get_feature_names_out(CAT_COLS))\n",
        "except Exception:\n",
        "    cat_names = [f\"{c}_{i}\" for i, c in enumerate(CAT_COLS)]\n",
        "feat_names = list(NUM_COLS) + cat_names\n",
        "\n",
        "clf = fitted[\"LogisticRegression\"].named_steps[\"clf\"]\n",
        "if hasattr(clf, \"coef_\"):\n",
        "    coefs = np.ravel(clf.coef_)\n",
        "    order = np.argsort(np.abs(coefs))[::-1][:25]\n",
        "    fig = plt.figure(figsize=(6,8)); ax = fig.add_subplot(111)\n",
        "    ax.barh(range(len(order)), np.abs(coefs[order])[::-1])\n",
        "    ax.set_yticks(range(len(order))); ax.set_yticklabels(np.array(feat_names)[order][::-1])\n",
        "    ax.set_title(\"Top |Coefficient| — Logistic Regression\"); ax.set_xlabel(\"Absolute Weight\")\n",
        "    fig.tight_layout(); fig.savefig(OUT_FIG / \"lr_top_coefs.png\"); plt.close(fig)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6) Ensembles\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Random Forest (coarse CV grid)\n",
        "rf = RandomForestClassifier(random_state=SEED, n_estimators=300, n_jobs=-1)\n",
        "rf_pipe = Pipeline([(\"pre\", pre), (\"clf\", rf)])\n",
        "param_grid = {\n",
        "    \"clf__max_depth\": [None, 10, 20],\n",
        "    \"clf__min_samples_split\": [2, 5, 10]\n",
        "}\n",
        "rf_cv = GridSearchCV(rf_pipe, param_grid, scoring=\"roc_auc\", cv=5, n_jobs=-1)\n",
        "rf_cv.fit(X_train, y_train)\n",
        "rf_best = rf_cv.best_estimator_\n",
        "\n",
        "if hasattr(rf_best.named_steps[\"clf\"], \"predict_proba\"):\n",
        "    y_score = rf_best.predict_proba(X_val)[:,1]\n",
        "else:\n",
        "    y_score = rf_best.decision_function(X_val)\n",
        "y_pred = (y_score >= 0.5).astype(int)\n",
        "m = dict(model=\"RandomForest\", roc_auc_val=roc_auc_score(y_val, y_score),\n",
        "         f1_val=f1_score(y_val, y_pred), precision_val=precision_score(y_val, y_pred, zero_division=0),\n",
        "         recall_val=recall_score(y_val, y_pred))\n",
        "fitted[\"RandomForest\"] = rf_best; val_rows.append(m)\n",
        "\n",
        "# Feature importance\n",
        "clf = rf_best.named_steps[\"clf\"]\n",
        "if hasattr(clf, \"feature_importances_\"):\n",
        "    imps = np.asarray(clf.feature_importances_)\n",
        "    order = np.argsort(imps)[::-1][:20]\n",
        "    fig = plt.figure(figsize=(6,8)); ax = fig.add_subplot(111)\n",
        "    ax.barh(range(len(order)), imps[order][::-1])\n",
        "    ax.set_yticks(range(len(order))); ax.set_yticklabels(np.array(feat_names)[order][::-1])\n",
        "    ax.set_title(\"Top Feature Importance — Random Forest\"); ax.set_xlabel(\"Importance\")\n",
        "    fig.tight_layout(); fig.savefig(OUT_FIG / \"feature_importance_top20.png\"); plt.close(fig)\n",
        "\n",
        "val_df = pd.DataFrame(val_rows).sort_values(\"roc_auc_val\", ascending=False)\n",
        "val_df.to_csv(OUT_TAB / \"validation_metrics.csv\", index=False)\n",
        "val_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# (Optional) Gradient Boosting — enable if course allows\n",
        "try:\n",
        "    gb = GradientBoostingClassifier(random_state=SEED)\n",
        "    gb_pipe = Pipeline([(\"pre\", pre), (\"clf\", gb)])\n",
        "    gb_params = {\"clf__learning_rate\":[0.05,0.1], \"clf__n_estimators\":[100,200], \"clf__max_depth\":[3]}\n",
        "    gb_cv = GridSearchCV(gb_pipe, gb_params, scoring=\"roc_auc\", cv=5, n_jobs=-1)\n",
        "    gb_cv.fit(X_train, y_train)\n",
        "    gb_best = gb_cv.best_estimator_\n",
        "    if hasattr(gb_best.named_steps[\"clf\"], \"predict_proba\"):\n",
        "        y_score = gb_best.predict_proba(X_val)[:,1]\n",
        "    else:\n",
        "        y_score = gb_best.decision_function(X_val)\n",
        "    y_pred = (y_score >= 0.5).astype(int)\n",
        "    m = dict(model=\"GradientBoosting\", roc_auc_val=roc_auc_score(y_val, y_score),\n",
        "             f1_val=f1_score(y_val, y_pred), precision_val=precision_score(y_val, y_pred, zero_division=0),\n",
        "             recall_val=recall_score(y_val, y_pred))\n",
        "    fitted[\"GradientBoosting\"] = gb_best; val_rows.append(m)\n",
        "except Exception as e:\n",
        "    print(\"GradientBoosting skipped:\", e)\n",
        "\n",
        "val_df = pd.DataFrame(val_rows).sort_values(\"roc_auc_val\", ascending=False)\n",
        "val_df.to_csv(OUT_TAB / \"validation_metrics.csv\", index=False)\n",
        "val_df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7) Neural network (shallow MLP)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "mlp = MLPClassifier(hidden_layer_sizes=(64,), activation=\"relu\", alpha=1e-4,\n",
        "                    early_stopping=True, random_state=SEED, max_iter=200)\n",
        "mlp_pipe = Pipeline([(\"pre\", pre), (\"clf\", mlp)])\n",
        "mlp_pipe, m = fit_eval(mlp_pipe, \"MLP(64)\")\n",
        "fitted[\"MLP\"] = mlp_pipe; val_rows.append(m)\n",
        "val_df = pd.DataFrame(val_rows).sort_values(\"roc_auc_val\", ascending=False)\n",
        "val_df.to_csv(OUT_TAB / \"validation_metrics.csv\", index=False)\n",
        "val_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Learning curve (5-fold CV) for MLP\n",
        "sizes, train_scores, val_scores = learning_curve(mlp_pipe, X_train, y_train, cv=5, scoring=\"roc_auc\",\n",
        "                                                 n_jobs=-1, train_sizes=np.linspace(0.2,1.0,5), shuffle=True, random_state=SEED)\n",
        "tr = train_scores.mean(axis=1); te = val_scores.mean(axis=1)\n",
        "fig = plt.figure(); ax = fig.add_subplot(111)\n",
        "ax.plot(sizes, tr, marker=\"o\", label=\"Train\")\n",
        "ax.plot(sizes, te, marker=\"o\", label=\"CV\")\n",
        "ax.set_title(\"Learning Curve — MLP\"); ax.set_xlabel(\"Train size\"); ax.set_ylabel(\"ROC-AUC\"); ax.legend(loc=\"best\")\n",
        "fig.tight_layout(); fig.savefig(OUT_FIG / \"mlp_learning_curve.png\"); plt.close(fig)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Pick best model by validation ROC-AUC\n",
        "val_df = pd.DataFrame(val_rows).sort_values(\"roc_auc_val\", ascending=False)\n",
        "best_name = val_df.iloc[0][\"model\"]\n",
        "best_pipe = fitted[best_name]\n",
        "\n",
        "# validation scores\n",
        "if hasattr(best_pipe.named_steps[\"clf\"], \"predict_proba\"):\n",
        "    y_score_val = best_pipe.predict_proba(X_val)[:,1]\n",
        "else:\n",
        "    y_score_val = best_pipe.decision_function(X_val)\n",
        "\n",
        "ths = np.linspace(0.1, 0.9, 33)\n",
        "rows = []\n",
        "for t in ths:\n",
        "    pred = (y_score_val >= t).astype(int)\n",
        "    rows.append({\n",
        "        \"threshold\": t,\n",
        "        \"f1\": f1_score(y_val, pred),\n",
        "        \"precision\": precision_score(y_val, pred, zero_division=0),\n",
        "        \"recall\": recall_score(y_val, pred)\n",
        "    })\n",
        "thr_df = pd.DataFrame(rows)\n",
        "thr_df.to_csv(OUT_TAB / \"threshold_sweep_val.csv\", index=False)\n",
        "\n",
        "fig = plt.figure(); ax = fig.add_subplot(111)\n",
        "ax.plot(thr_df[\"threshold\"], thr_df[\"precision\"], label=\"Precision\")\n",
        "ax.plot(thr_df[\"threshold\"], thr_df[\"recall\"], label=\"Recall\")\n",
        "ax.plot(thr_df[\"threshold\"], thr_df[\"f1\"], label=\"F1\")\n",
        "ax.set_xlabel(\"Threshold\"); ax.set_ylabel(\"Score\")\n",
        "ax.set_title(f\"Threshold Sweep — {best_name} (Validation)\"); ax.legend(loc=\"best\")\n",
        "fig.tight_layout(); fig.savefig(OUT_FIG / \"threshold_sweep.png\"); plt.close(fig)\n",
        "\n",
        "best_thr = float(thr_df.sort_values(\"f1\", ascending=False).iloc[0][\"threshold\"])\n",
        "(Path(OUT_TAB / \"chosen_threshold.json\")).write_text(json.dumps({\"model\": best_name, \"threshold\": best_thr}, indent=2))\n",
        "best_name, best_thr\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9) Test evaluation & calibration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_name = json.loads((OUT_TAB / \"chosen_threshold.json\").read_text())[\"model\"]\n",
        "best_thr = json.loads((OUT_TAB / \"chosen_threshold.json\").read_text())[\"threshold\"]\n",
        "pipe = fitted[best_name]\n",
        "\n",
        "# Scores\n",
        "if hasattr(pipe.named_steps[\"clf\"], \"predict_proba\"):\n",
        "    y_score_test = pipe.predict_proba(X_test)[:,1]\n",
        "else:\n",
        "    y_score_test = pipe.decision_function(X_test)\n",
        "\n",
        "y_pred_test = (y_score_test >= best_thr).astype(int)\n",
        "\n",
        "test_metrics = {\n",
        "    \"model\": best_name,\n",
        "    \"threshold\": best_thr,\n",
        "    \"roc_auc_test\": roc_auc_score(y_test, y_score_test),\n",
        "    \"f1_test\": f1_score(y_test, y_pred_test),\n",
        "    \"precision_test\": precision_score(y_test, y_pred_test, zero_division=0),\n",
        "    \"recall_test\": recall_score(y_test, y_pred_test)\n",
        "}\n",
        "pd.DataFrame([test_metrics]).to_csv(OUT_TAB / \"test_metrics.csv\", index=False)\n",
        "print(test_metrics)\n",
        "\n",
        "# ROC\n",
        "fpr, tpr, _ = roc_curve(y_test, y_score_test)\n",
        "fig = plt.figure(); ax = fig.add_subplot(111)\n",
        "ax.plot(fpr, tpr, label=f\"AUC={auc(fpr,tpr):.3f}\")\n",
        "ax.plot([0,1],[0,1],\"--\")\n",
        "ax.set_title(f\"ROC Curve — Test ({best_name})\"); ax.set_xlabel(\"FPR\"); ax.set_ylabel(\"TPR\"); ax.legend(loc=\"lower right\")\n",
        "fig.tight_layout(); fig.savefig(OUT_FIG / \"roc_curve_test.png\"); plt.close(fig)\n",
        "\n",
        "# PR\n",
        "prec, rec, _ = precision_recall_curve(y_test, y_score_test)\n",
        "fig = plt.figure(); ax = fig.add_subplot(111)\n",
        "ax.plot(rec, prec)\n",
        "ax.set_title(\"Precision–Recall Curve — Test\"); ax.set_xlabel(\"Recall\"); ax.set_ylabel(\"Precision\")\n",
        "fig.tight_layout(); fig.savefig(OUT_FIG / \"pr_curve_test.png\"); plt.close(fig)\n",
        "\n",
        "# Confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred_test)\n",
        "fig = plt.figure(); ax = fig.add_subplot(111)\n",
        "ax.imshow(cm, interpolation=\"nearest\")\n",
        "ax.set_title(\"Confusion Matrix — Test\"); ax.set_xlabel(\"Predicted\"); ax.set_ylabel(\"Actual\")\n",
        "for (i,j), v in np.ndenumerate(cm):\n",
        "    ax.text(j, i, str(v), ha=\"center\", va=\"center\")\n",
        "fig.tight_layout(); fig.savefig(OUT_FIG / \"confusion_matrix_test.png\"); plt.close(fig)\n",
        "\n",
        "# Calibration / Reliability & Brier\n",
        "prob_true, prob_pred = calibration_curve(y_test, y_score_test, n_bins=10, strategy=\"quantile\")\n",
        "fig = plt.figure(); ax = fig.add_subplot(111)\n",
        "ax.plot(prob_pred, prob_true, marker=\"o\"); ax.plot([0,1],[0,1],\"--\")\n",
        "ax.set_title(\"Reliability (Calibration) — Test\"); ax.set_xlabel(\"Predicted probability\"); ax.set_ylabel(\"Observed frequency\")\n",
        "brier = brier_score_loss(y_test, y_score_test); ax.text(0.6, 0.1, f\"Brier={brier:.3f}\", transform=ax.transAxes)\n",
        "fig.tight_layout(); fig.savefig(OUT_FIG / \"calibration_curve_test.png\"); plt.close(fig)\n",
        "(OUT_TAB / \"brier_score.txt\").write_text(str(brier))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10) Interpretability\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Expanded feature names from fitted preprocessor\n",
        "pre_fit = pre.fit(X_train, y_train)\n",
        "try:\n",
        "    cat_names = list(pre_fit.named_transformers_[\"cat\"].get_feature_names_out(CAT_COLS))\n",
        "except Exception:\n",
        "    cat_names = [f\"{c}_{i}\" for i, c in enumerate(CAT_COLS)]\n",
        "feat_names = np.array(list(NUM_COLS) + cat_names)\n",
        "\n",
        "pipe = fitted[best_name]\n",
        "clf = pipe.named_steps[\"clf\"]\n",
        "vals = None; label = \"Importance\"\n",
        "if hasattr(clf, \"feature_importances_\"):\n",
        "    vals = np.asarray(clf.feature_importances_); label = \"Feature Importance\"\n",
        "elif hasattr(clf, \"coef_\"):\n",
        "    vals = np.abs(np.ravel(clf.coef_)); label = \"|Coefficient|\"\n",
        "\n",
        "if vals is not None:\n",
        "    order = np.argsort(vals)[::-1][:25]\n",
        "    fig = plt.figure(figsize=(6,8)); ax = fig.add_subplot(111)\n",
        "    ax.barh(range(len(order)), vals[order][::-1])\n",
        "    ax.set_yticks(range(len(order))); ax.set_yticklabels(feat_names[order][::-1])\n",
        "    ax.set_title(f\"Top {label} — {best_name}\"); ax.set_xlabel(label)\n",
        "    fig.tight_layout(); fig.savefig(OUT_FIG / \"feature_importance_top25.png\"); plt.close(fig)\n",
        "else:\n",
        "    print(\"Model does not expose feature importance or coefficients.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11) Ablations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "BEHAVIORAL = [c for c in [\n",
        "    \"Administrative\",\"Administrative_Duration\",\"Informational\",\"Informational_Duration\",\n",
        "    \"ProductRelated\",\"ProductRelated_Duration\",\"BounceRates\",\"ExitRates\",\"PageValues\",\"SpecialDay\"\n",
        "] if c in X.columns]\n",
        "\n",
        "TEMPORAL_TECH = [c for c in [\n",
        "    \"Month\",\"Weekend\",\"OperatingSystems\",\"Browser\",\"Region\",\"TrafficType\",\"VisitorType\"\n",
        "] if c in X.columns]\n",
        "\n",
        "def make_pre(cols_keep):\n",
        "    num = [c for c in cols_keep if c in NUM_COLS]\n",
        "    cat = [c for c in cols_keep if c in CAT_COLS]\n",
        "    return ColumnTransformer([(\"num\", StandardScaler(), num), (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat)])\n",
        "\n",
        "def eval_group(cols_keep, label):\n",
        "    pre_g = make_pre(cols_keep)\n",
        "    # reuse best model family for fairness\n",
        "    if best_name.startswith(\"RandomForest\"):\n",
        "        clf = RandomForestClassifier(random_state=SEED, n_estimators=300, n_jobs=-1)\n",
        "    elif best_name.startswith(\"GradientBoosting\") or best_name == \"GradientBoosting\":\n",
        "        clf = GradientBoostingClassifier(random_state=SEED)\n",
        "    elif best_name.startswith(\"MLP\") or best_name == \"MLP(64)\":\n",
        "        clf = MLPClassifier(hidden_layer_sizes=(64,), activation=\"relu\", alpha=1e-4, early_stopping=True, random_state=SEED, max_iter=200)\n",
        "    elif best_name.startswith(\"Logistic\") or best_name == \"LogisticRegression\":\n",
        "        clf = LogisticRegression(max_iter=1000, class_weight=\"balanced\", random_state=SEED)\n",
        "    else:\n",
        "        clf = RandomForestClassifier(random_state=SEED, n_estimators=300, n_jobs=-1)\n",
        "    pipe = Pipeline([(\"pre\", pre_g), (\"clf\", clf)])\n",
        "    _, m = fit_eval(pipe, f\"{best_name} | {label}\")\n",
        "    return m\n",
        "\n",
        "abl_rows = []\n",
        "# Full\n",
        "full_row = [r for r in val_rows if r[\"model\"]==best_name]\n",
        "if full_row:\n",
        "    abl_rows.append({\"group\":\"Full\", \"roc_auc_val\": full_row[0][\"roc_auc_val\"]})\n",
        "\n",
        "if BEHAVIORAL:\n",
        "    m = eval_group(BEHAVIORAL, \"BehavioralOnly\")\n",
        "    abl_rows.append({\"group\":\"BehavioralOnly\", \"roc_auc_val\": m[\"roc_auc_val\"]})\n",
        "\n",
        "if TEMPORAL_TECH:\n",
        "    m = eval_group(TEMPORAL_TECH, \"TemporalTechOnly\")\n",
        "    abl_rows.append({\"group\":\"TemporalTechOnly\", \"roc_auc_val\": m[\"roc_auc_val\"]})\n",
        "\n",
        "abl_df = pd.DataFrame(abl_rows)\n",
        "abl_df.to_csv(OUT_TAB / \"ablations.csv\", index=False)\n",
        "\n",
        "if \"Full\" in abl_df[\"group\"].values:\n",
        "    base = float(abl_df.loc[abl_df[\"group\"]==\"Full\",\"roc_auc_val\"])\n",
        "    abl_df2 = abl_df.copy()\n",
        "    abl_df2[\"delta_auc\"] = abl_df2[\"roc_auc_val\"] - base\n",
        "    ax = abl_df2.set_index(\"group\")[\"delta_auc\"].plot(kind=\"bar\", title=\"Ablation ΔAUC vs Full\")\n",
        "    fig = ax.get_figure(); fig.tight_layout(); fig.savefig(OUT_FIG / \"ablation_delta_auc.png\"); plt.close(fig)\n",
        "abl_df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12) Business takeaways (fill 5–7 bullets)\n",
        "- Signals that most increase purchase likelihood  \n",
        "- Recommended operating threshold and trade-offs  \n",
        "- Implications for remarketing and UX\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 13) Conclusion & future direction\n",
        "- Best model and trade-offs  \n",
        "- Future: calibration tuning, cost-sensitive metrics, RFM enrichment, deployment hooks\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 14) Optional RFM (Online Retail II)\n",
        "Hook for building Recency, Frequency, Monetary features and joining if a key exists.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 15) Export helpers & final assets\n",
        "def _save(fig, name):\n",
        "    path = (OUT_FIG / f\"{name}.png\")\n",
        "    fig.savefig(path, dpi=300, bbox_inches=\"tight\"); plt.close(fig); return str(path)\n",
        "\n",
        "def save_confusion_matrix(model, X, y, name):\n",
        "    y_hat = model.predict(X)\n",
        "    cm = confusion_matrix(y, y_hat)\n",
        "    fig = plt.figure(); ax = fig.add_subplot(111)\n",
        "    ax.imshow(cm, interpolation=\"nearest\"); ax.set_title(\"Confusion Matrix\")\n",
        "    ax.set_xlabel(\"Predicted\"); ax.set_ylabel(\"Actual\")\n",
        "    for (i,j), v in np.ndenumerate(cm): ax.text(j, i, str(v), ha=\"center\", va=\"center\")\n",
        "    return _save(fig, name)\n",
        "\n",
        "def save_roc(model, X, y, name):\n",
        "    if hasattr(model.named_steps[\"clf\"], \"predict_proba\"): y_score = model.predict_proba(X)[:,1]\n",
        "    else: y_score = model.decision_function(X)\n",
        "    fpr, tpr, _ = roc_curve(y, y_score)\n",
        "    fig = plt.figure(); ax = fig.add_subplot(111)\n",
        "    ax.plot(fpr, tpr, label=f\"AUC={auc(fpr,tpr):.3f}\"); ax.plot([0,1],[0,1],\"--\")\n",
        "    ax.set_title(\"ROC Curve\"); ax.set_xlabel(\"FPR\"); ax.set_ylabel(\"TPR\"); ax.legend(loc=\"lower right\")\n",
        "    return _save(fig, name)\n",
        "\n",
        "def save_pr(model, X, y, name):\n",
        "    if hasattr(model.named_steps[\"clf\"], \"predict_proba\"): y_score = model.predict_proba(X)[:,1]\n",
        "    else: y_score = model.decision_function(X)\n",
        "    precision, recall, _ = precision_recall_curve(y, y_score)\n",
        "    fig = plt.figure(); ax = fig.add_subplot(111)\n",
        "    ax.plot(recall, precision); ax.set_title(\"Precision–Recall Curve\"); ax.set_xlabel(\"Recall\"); ax.set_ylabel(\"Precision\")\n",
        "    return _save(fig, name)\n",
        "\n",
        "def save_calibration(model, X, y, name):\n",
        "    if hasattr(model.named_steps[\"clf\"], \"predict_proba\"): y_score = model.predict_proba(X)[:,1]\n",
        "    else:\n",
        "        y_score = model.decision_function(X)\n",
        "        y_score = (y_score - y_score.min()) / (y_score.max() - y_score.min() + 1e-12)\n",
        "    prob_true, prob_pred = calibration_curve(y, y_score, n_bins=10, strategy=\"quantile\")\n",
        "    fig = plt.figure(); ax = fig.add_subplot(111)\n",
        "    ax.plot(prob_pred, prob_true, marker=\"o\"); ax.plot([0,1],[0,1],\"--\")\n",
        "    ax.set_title(\"Reliability (Calibration)\"); ax.set_xlabel(\"Predicted probability\"); ax.set_ylabel(\"Observed frequency\")\n",
        "    brier = brier_score_loss(y, y_score); ax.text(0.6, 0.1, f\"Brier={brier:.3f}\", transform=ax.transAxes)\n",
        "    return _save(fig, name)\n",
        "\n",
        "def save_feature_importance_fig(model, feat_names, name):\n",
        "    clf = model.named_steps[\"clf\"]\n",
        "    vals = None; label=\"Importance\"\n",
        "    if hasattr(clf, \"feature_importances_\"):\n",
        "        vals = np.asarray(clf.feature_importances_); label=\"Feature Importance\"\n",
        "    elif hasattr(clf, \"coef_\"):\n",
        "        vals = np.abs(np.ravel(clf.coef_)); label=\"|Coefficient|\"\n",
        "    if vals is None: return None\n",
        "    order = np.argsort(vals)[::-1][:25]\n",
        "    fig = plt.figure(figsize=(6,8)); ax = fig.add_subplot(111)\n",
        "    ax.barh(range(len(order)), vals[order][::-1])\n",
        "    ax.set_yticks(range(len(order))); ax.set_yticklabels(np.array(feat_names)[order][::-1])\n",
        "    ax.set_title(f\"Top {label}\"); ax.set_xlabel(label)\n",
        "    return _save(fig, name)\n",
        "\n",
        "def export_all(model, X_test, y_test, feat_names, prefix=\"purchase_pred\"):\n",
        "    paths = {}\n",
        "    paths[\"roc\"] = save_roc(model, X_test, y_test, f\"{prefix}_roc_test\")\n",
        "    paths[\"pr\"] = save_pr(model, X_test, y_test, f\"{prefix}_pr_test\")\n",
        "    paths[\"cm\"] = save_confusion_matrix(model, X_test, y_test, f\"{prefix}_cm_test\")\n",
        "    paths[\"cal\"] = save_calibration(model, X_test, y_test, f\"{prefix}_calibration_test\")\n",
        "    fi = save_feature_importance_fig(model, feat_names, f\"{prefix}_feature_importance\")\n",
        "    if fi: paths[\"feature_importance\"] = fi\n",
        "    rep = classification_report(y_test, model.predict(X_test), output_dict=True)\n",
        "    (OUT_TAB / f\"{prefix}_classification_report.json\").write_text(json.dumps(rep, indent=2))\n",
        "    return paths\n",
        "\n",
        "# Expanded names for feature-importance figure\n",
        "pre_fit = pre.fit(X_train, y_train)\n",
        "try:\n",
        "    cat_names = list(pre_fit.named_transformers_[\"cat\"].get_feature_names_out(CAT_COLS))\n",
        "except Exception:\n",
        "    cat_names = [f\"{c}_{i}\" for i, c in enumerate(CAT_COLS)]\n",
        "feat_names = list(NUM_COLS) + cat_names\n",
        "\n",
        "export_paths = export_all(fitted[best_name], X_test, y_test, feat_names, prefix=\"purchase_pred\")\n",
        "(OUT_TAB / \"export_paths.json\").write_text(json.dumps(export_paths, indent=2, ensure_ascii=False))\n",
        "\n",
        "# Collect a curated set for slides/poster\n",
        "ASSET_DIR = ROOT / \"presentations\" / \"assets\"\n",
        "ASSET_DIR.mkdir(parents=True, exist_ok=True)\n",
        "ordered = [\"class_counts.png\",\"threshold_sweep.png\",\"roc_curve_test.png\",\"confusion_matrix_test.png\",\n",
        "           \"feature_importance_top20.png\",\"feature_importance_top25.png\",\"mlp_learning_curve.png\",\"ablation_delta_auc.png\"]\n",
        "i = 1\n",
        "for name in ordered:\n",
        "    src = OUT_FIG / name\n",
        "    if src.exists():\n",
        "        (ASSET_DIR / f\"{i:02d}_{name}\").write_bytes(src.read_bytes()); i += 1\n",
        "print(\"Assets exported →\", ASSET_DIR)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
